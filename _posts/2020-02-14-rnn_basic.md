# RNN_Pytorch 初步

Pytorch学习笔记系列。

RNN主要用作时序（或者序列）数据如语音，文本等的处理。本文以语言模型为例，初步介绍使用RNN的基本知识。本文目录：

1. TOC
{:toc}

## 一、语言模型

给定一个长度为$T$的词的序列$w_1, w_2, \ldots, w_T$，语言模型将计算该序列的概率：

$$P(w_1, w_2, \ldots, w_T).$$

假设序列$w_1, w_2, \ldots, w_T$中的每个词是依次生成的，我们有

$$P(w_1, w_2, \ldots, w_T) = \prod_{t=1}^T P(w_t \mid w_1, \ldots, w_{t-1}).$$

计算太复杂，所以有n-gram模型。考虑马尔可夫假设，即一个词的出现只与前面$n$个词相关，即$n$阶马尔可夫链（Markov chain of order $n$）。如果基于$n-1$阶马尔可夫链，我们可以将语言模型改写为

$$P(w_1, w_2, \ldots, w_T) \approx \prod_{t=1}^T P(w_t \mid w_{t-(n-1)}, \ldots, w_{t-1}) .$$


以上也叫$n$元语法（$n$-grams）。它是基于$n - 1$阶马尔可夫链的概率语言模型。当$n$分别为1、2和3时，我们将其分别称作一元语法（unigram）、二元语法（bigram）和三元语法（trigram）。例如，长度为4的序列$w_1, w_2, w_3, w_4$在一元语法、二元语法和三元语法中的概率分别为

$$
\begin{aligned}
P(w_1, w_2, w_3, w_4) &=  P(w_1) P(w_2) P(w_3) P(w_4) ,\\
P(w_1, w_2, w_3, w_4) &=  P(w_1) P(w_2 \mid w_1) P(w_3 \mid w_2) P(w_4 \mid w_3) ,\\
P(w_1, w_2, w_3, w_4) &=  P(w_1) P(w_2 \mid w_1) P(w_3 \mid w_1, w_2) P(w_4 \mid w_2, w_3) .
\end{aligned}
$$
**$N$元语法是基于$n-1$阶马尔可夫链的概率语言模型，其中$n$权衡了计算复杂度和模型准确性。**

## 二、循环神经网络
假设 $\boldsymbol{X}_t\in\mathbb{R}^{n\times d}$ 是序列中时间步 $t$ 的小批量输入， $\boldsymbol{H}_t\in\mathbb{R}^{n\times  h}$ 是该时间步的隐藏变量。 与多层感知机不同的是，这里我们保存上一时间步的隐藏变量 $\boldsymbol{H}_{t-1}$，并引入一个新的权重参数 $\boldsymbol{W}_{hh} \in \mathbb{R}^{h \times h}$，该参数用来描述在当前时间步如何使用上一时间步的隐藏变量。

具体来说，时间步$t$的隐藏变量的计算由当前时间步的输入和上一时间步的隐藏变量共同决定：

$$
\boldsymbol{H}_t = \phi(\boldsymbol{X}_t \boldsymbol{W}_{xh} + \boldsymbol{H}_{t-1} \boldsymbol{W}_{hh}  + \boldsymbol{b}_h).
$$

![](/images/rnn_basic/rnn.svg)

## 三、语言模型的数据准备

### 1. 建立字符索引

将每个字符映射成一个从0开始的连续整数，又称索引，来方便之后的数据处理。为了得到索引，我们将数据集里所有不同字符取出来，然后将其逐一映射到索引来构造词典。接着，打印`vocab_size`，即词典中不同字符的个数，又称词典大小。

``` python
#假设读入的字符数据在 corpus_chars 中
idx_to_char = list(set(corpus_chars)) #利用集合建立索引->字符，也称词典
char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)]) ##字符->索引
vocab_size = len(char_to_idx) ##词典的大小

```

之后，将训练数据集中每个字符转化为索引。

``` python
# 训练数据集中每个字符转化为索引 在 corpus_indices 中
corpus_indices = [char_to_idx[char] for char in corpus_chars] 

## 测试，并打印前20个字符及其对应的索引。
sample = corpus_indices[:20]
print('chars:', ''.join([idx_to_char[idx] for idx in sample]))
print('indices:', sample)
```

### 2. 时序数据采样

+ 随机采样

  每次从数据里随机采样一个小批量。其中批量大小`batch_size`指每个小批量的样本数，`num_steps`为每个样本所包含的时间步数。
  在随机采样中，每个样本是原始序列上任意截取的一段序列。相邻的两个随机小批量在原始序列上的位置不一定相毗邻。在训练模型时，每次随机采样前都需要重新初始化隐藏状态。

  ```python
  def data_iter_random(corpus_indices, batch_size, num_steps, device=None):
      # 减1是因为输出的索引x是相应输入的索引y加1
      num_examples = (len(corpus_indices) - 1) // num_steps
      epoch_size = num_examples // batch_size
      example_indices = list(range(num_examples))
      random.shuffle(example_indices)
  
      # 返回从pos开始的长为num_steps的序列
      def _data(pos):
          return corpus_indices[pos: pos + num_steps]
      if device is None:
          device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
      
      for i in range(epoch_size):
          # 每次读取batch_size个随机样本
          i = i * batch_size
          batch_indices = example_indices[i: i + batch_size]
          X = [_data(j * num_steps) for j in batch_indices]
          Y = [_data(j * num_steps + 1) for j in batch_indices]
          yield torch.tensor(X, dtype=torch.float32, device=device), torch.tensor(Y, dtype=torch.float32, device=device)
  ```

  

+ 相邻采样

  + 令相邻的两个随机小批量在原始序列上的位置相毗邻。这时候，我们就可以用一个小批量最终时间步的隐藏状态来初始化下一个小批量的隐藏状态，从而使下一个小批量的输出也取决于当前小批量的输入，并如此循环下去。

  ```python
  def data_iter_consecutive(corpus_indices, batch_size, num_steps, device=None):
      if device is None:
          device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
      corpus_indices = torch.tensor(corpus_indices, dtype=torch.float32, device=device)
      data_len = len(corpus_indices)
      batch_len = data_len // batch_size
      indices = corpus_indices[0: batch_size*batch_len].view(batch_size, batch_len)
      epoch_size = (batch_len - 1) // num_steps
      for i in range(epoch_size):
          i = i * num_steps
          X = indices[:, i: i + num_steps]
          Y = indices[:, i + 1: i + num_steps + 1]
          yield X, Y
  ```

  ### 3. one-hot向量

  为了将词表示成向量输入到神经网络，一个简单的办法是使用one-hot向量。假设词典中不同字符的数量为$N$（即词典大小`vocab_size`），每个字符已经同一个从0到$N-1$的连续整数值索引一一对应。如果一个字符的索引是整数$i$, 那么我们创建一个全0的长为$N$的向量，并将其位置为$i$的元素设成1。该向量就是对原字符的one-hot向量。

  ```python
  # pytorch实现
  torch.nn.functional.one_hot(tensor, num_classes=[...])
  ##基本原理类似以下的实现
  def one_hot(x, n_class, dtype=torch.float32): 
      # X shape: (batch), output shape: (batch, n_class)
      x = x.long()
      res = torch.zeros(x.shape[0], n_class, dtype=dtype, device=x.device)
      res.scatter_(1, x.view(-1, 1), 1)
      return res
  
  ```

  从步骤2采样的得到小批量的训练数据$[X,y]$中$X$的形状是*(batch, seq_len )*。下面的函数将这样的小批量变换成数个可以输入进网络的形状为 seq_len 个(batch, vocab_size)的矩阵。也就是说，每个时间步$t$的输入为$\boldsymbol{X}_t \in \mathbb{R}^{n \times d}$，其中$n$为批量大小，$d$为输入个数，即one-hot向量长度（词典大小）。

  ``` python
  def to_onehot(X, n_class):  
      # X shape: (batch, seq_len), output: seq_len elements of (batch, n_class)
      return [one_hot(X[:, i], n_class) for i in range(X.shape[1])]
  
  ```

  经过这样处理后，就可以作为RNN的输入了。

  

## 四、pytorch RNN基础

Pytorch中的`nn.RNN`来构造循环神经网络。

```python
rnn=torch.nn.RNN(**input_size**: int, hidden_size: int, num_layers: int, bias: bool, batch_first: bool, dropout: float, bidirectional: bool, nonlinearity: str) -> None
```

主要关注`nn.RNN`的以下几个构造函数参数：

* `input_size` - The number of expected features in the input x
* `hidden_size` – The number of features in the hidden state h
* `nonlinearity` – The non-linearity to use. Can be either 'tanh' or 'relu'. Default: 'tanh'
* `batch_first` – If True, then the input and output tensors are provided as (batch_size, num_steps, input_size). Default: False

这里的`batch_first`决定了输入的形状，我们使用默认的参数`False`，对应的输入形状是 (num_steps, batch_size, input_size)。

`rnn.forward(**input**, hx: ...) -> Tensor`函数的参数为：

* `input` of shape (num_steps, batch_size, input_size): tensor containing the features of the input sequence. 
* `h_0` of shape (num_layers * num_directions, batch_size, hidden_size): tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided. If the RNN is bidirectional, num_directions should be 2, else it should be 1.

`forward`函数的返回值是：

* `output` of shape (num_steps, batch_size, num_directions * hidden_size): tensor containing the output features (h_t) from the last layer of the RNN, for each t.
* `h_n` of shape (num_layers * num_directions, batch_size, hidden_size): tensor containing the hidden state for t = num_steps.

现在我们构造一个`nn.RNN`实例，并用一个简单的例子来看一下输出的形状。

```python
rnn_layer = nn.RNN(input_size=vocab_size, hidden_size=num_hiddens)
num_steps, batch_size = 35, 2
X = torch.rand(num_steps, batch_size, vocab_size)
state = None
Y, state_new = rnn_layer(X, state)
print(Y.shape, state_new.shape)
# torch.Size([35, 2, 256]) torch.Size([1, 2, 256])
```

## 五、基于RNN的语言模型的pytorch实现

1. 一个完整的完整的基于循环神经网络的语言模型。

   ```python
     class RNNModel(nn.Module):
           def __init__(self, rnn_layer, vocab_size):
               super(RNNModel, self).__init__()
               self.rnn = rnn_layer
               self.hidden_size = rnn_layer.hidden_size * (2 if rnn_layer.bidirectional else 1) 
               self.vocab_size = vocab_size
               self.dense = nn.Linear(self.hidden_size, vocab_size)
   
           def forward(self, inputs, state):
               # inputs.shape: (batch_size, num_steps)
               X = to_onehot(inputs, vocab_size)
               X = torch.stack(X)  # X.shape: (num_steps, batch_size, vocab_size)
               hiddens, state = self.rnn(X, state)
               hiddens = hiddens.view(-1, hiddens.shape[-1])  # hiddens.shape: (num_steps * batch_size, hidden_size)
               output = self.dense(hiddens)
               return output, state
   ```

2. 训练和预测函数

   ```python
   def train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device,corpus_indices, idx_to_char, char_to_idx,
    num_epochs, num_steps, lr, clipping_theta,batch_size, pred_period, pred_len, prefixes):
       loss = nn.CrossEntropyLoss()
       optimizer = torch.optim.Adam(model.parameters(), lr=lr)
       model.to(device)
       for epoch in range(num_epochs):
           l_sum, n, start = 0.0, 0, time.time()
           data_iter = d2l.data_iter_consecutive(corpus_indices, batch_size, num_steps, device) # 相邻采样
           state = None
           for X, Y in data_iter:
               if state is not None:
                   # 使用detach函数从计算图分离隐藏状态
                   if isinstance (state, tuple): # LSTM, state:(h, c)  
                       state[0].detach_()
                       state[1].detach_()
                   else: 
                       state.detach_()
               (output, state) = model(X, state) # output.shape: (num_steps * batch_size, vocab_size)
               y = torch.flatten(Y.T)
               l = loss(output, y.long())
               
               optimizer.zero_grad()
               l.backward()
               grad_clipping(model.parameters(), clipping_theta, device)
               optimizer.step()
               l_sum += l.item() * y.shape[0]
               n += y.shape[0]
           
   		## 预测 （如果不需要，可以用作验证训练结果）
           if (epoch + 1) % pred_period == 0:
               print('epoch %d, perplexity %f, time %.2f sec' % (epoch + 1, math.exp(l_sum / n), time.time() - start))
               for prefix in prefixes:
                   print(' -', predict_rnn_pytorch(
                       prefix, pred_len, model, vocab_size, device, idx_to_char, char_to_idx))
   ```

   进行训练：

   ```python
   num_epochs, batch_size, lr, clipping_theta = 250, 32, 1e-3, 1e-2
   pred_period, pred_len, prefixes = 50, 50, ['分开', '不分开']
   train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device,corpus_indices, idx_to_char, char_to_idx,num_epochs, num_steps, lr, clipping_theta, batch_size, pred_period, pred_len, prefixes)
   ```

   

3. 预测函数

   实现一个预测函数，与前面的模型的区别在于前向计算和初始化隐藏状态。

   ```python
   def predict_rnn_pytorch(prefix, num_chars, model, vocab_size, device, idx_to_char,
                         char_to_idx):
       state = None
       output = [char_to_idx[prefix[0]]]  # output记录prefix加上预测的num_chars个字符
       for t in range(num_chars + len(prefix) - 1):
           X = torch.tensor([output[-1]], device=device).view(1, 1)
           (Y, state) = model(X, state)  # 前向计算不需要传入模型参数
           if t < len(prefix) - 1:
               output.append(char_to_idx[prefix[t + 1]])
           else:
               output.append(Y.argmax(dim=1).item())
       return ''.join([idx_to_char[i] for i in output])
   ```

   使用方法：

   ```python
   model = RNNModel(rnn_layer, vocab_size).to(device) 
   # 在此可以调入已训练的模型
   predict_rnn_pytorch('分开', 10, model, vocab_size, device, idx_to_char, char_to_idx)
   ```