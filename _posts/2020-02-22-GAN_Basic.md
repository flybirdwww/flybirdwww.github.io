
# 对抗生成网络基础 

Pytorch学习笔记系列。

对抗生成网络(Generative Adversarial Networks,GANs)是一类新的生成模型。这里作简单介绍（来自于[伯禹学习平台](https://www.boyuai.com/)）。本文目录：

1. TOC
{:toc}


## 对抗生成网络Generative Adversarial Networks

2014年，Goodfellow提出了Generative adversarial networks (GANs)。GANs的核心思想是，如果我们不能区分假数据和真实数据，那么数据生成器就是好的。在统计中，这被称为双样本测试—用于回答数据集$X=\{x_1，\ldots, x_n\}$和是$X'=\{x'_1,\ldots, x'_n\}$否来自相同的分布的问题。大多数统计论文和GANs的主要区别在于后者以一种建设性的方式使用了这个概念。换句话说，他们不只是训练一个模型说“嘿，这两个数据集看起来不像是来自同一个分布”，而是使用[双样本测试](https://en.wikipedia.org/wiki/two - sample_is_testing)向生成模型提供训练信号。这允许我们改进数据生成器，直到它生成类似于真实数据的内容。至少，它需要欺骗分类器。即使我们的分类器是最先进的深度神经网络。




![Image Name](/images/GAN_Basic/gan_1.jpg)



GAN的体系结构如图所示。正如你所看到的，在GAN架构中有两个部分——首先，我们需要一个设备(比如，一个深度网络，但它实际上可以是任何东西，比如游戏渲染引擎)，它可能能够生成看起来像真实的数据。如果我们是处理图像，它就需要生成图像。如果我们处理语音，它需要生成音频序列，等等。我们称之为生成器网络。第二部分是判别器网络。它试图区分虚假数据和真实数据。这两个网络都在互相竞争。发生器网络试图欺骗鉴别器网络。这时，鉴别器网络就会适应新的假数据。这些信息反过来又用于改进发生器网络，如此反复。

鉴别器是一个二进制分类器，用于区分输入$x$是真(来自真实数据)还是假(来自生成器)。通常，针对输入$\mathbf x$识别器输出一个标量预测$o\in\mathbb R$  ，例如使用一个隐藏大小为1的稠密层，然后应用sigmoid函数获得预测概率$D(\mathbf x) = 1/(1+e^{-o})$。假设标签$y$表示真数据，$1$表示假数据，$0$表示假数据。我们训练鉴别器最小化交叉熵损失，i.e., 

$$
 \min_D \{ - y \log D(\mathbf x) - (1-y)\log(1-D(\mathbf x)) \},
$$


对于生成器，它首行从参数 $\mathbf z\in\mathbb R^d$ 得到随机数据， *e.g.*, 一个正态分布 $\mathbf z \sim \mathcal{N} (0, 1)$. 我们经常称 $\mathbf z$ 为隐变量( latent variable). z 输入一个函数中得到 $\mathbf x'=G(\mathbf z)$. 生成器的目的就是要欺骗判断器，让生成器认为 $\mathbf x'=G(\mathbf z)$ 是真实数据, *i.e.*, 即我们想要 $D( G(\mathbf z)) \approx 1$.换句话说, 对于给定的判断器 $D$, 我们更新生成器 $G$ 的参数据最大化  当当 $y=0$时的cross-entropy loss , *i.e.*,


$$
\max_G \{ - (1-y) \log(1-D(G(\mathbf z))) \} = \max_G \{ - \log(1-D(G(\mathbf z))) \}.
$$



如果鉴别器做得很好，那么$D(\mathbf x')\approx 0$，因此上述损失接近0，这导致梯度太小，不能使生成器取得良好的进展。所以通常我们会尽量减少以下损失:

$$
 \min_G \{ - y \log(D(G(\mathbf z))) \} = \min_G \{ - \log(D(G(\mathbf z))) \}, 
$$


which is just feed $\mathbf x'=G(\mathbf z)$ into the discriminator but giving label $y=1$.


To sum up, $D$ and $G$ are playing a "minimax" game with the comprehensive objective function:


$$
min_D max_G \{ -E_{x \sim \text{Data}} log D(\mathbf x) - E_{z \sim \text{Noise}} log(1 - D(G(\mathbf z))) \}.
$$




许多GANs应用程序都是在图像上下文中运行的。作为演示目的，我们将首先拟合一个更简单的本。我们将说明如果我们使用GANs来构建世界上最低效的高斯分布参数估计器。让我们开始吧。


```python
%matplotlib inline
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader
from torch import nn
import numpy as np
from torch.autograd import Variable
import torch
```

###  生成一些“真实”数据

举个简单的示例，我们从高斯函数中生成数据。


```python
X=np.random.normal(size=(1000,2))
A=np.array([[1,2],[-0.1,0.5]])
b=np.array([1,2])
data=X.dot(A)+b
```

我们得到的是一个均值为 $b$, 协方差矩阵为 $A^TA$的二维高斯数据。 


```python
plt.figure(figsize=(3.5,2.5))
plt.scatter(X[:100,0],X[:100,1],color='red')
plt.show()
plt.figure(figsize=(3.5,2.5))
plt.scatter(data[:100,0],data[:100,1],color='blue')
plt.show()
print("The covariance matrix is\n%s" % np.dot(A.T, A))
```

<img src="/images/GAN_Basic/gan_2.png">



<img src="https://cdn.kesci.com/rt_upload/D794A0FAF6C74E17AF54E8636B5A7B11/q5tv55ag0i.png">


    The covariance matrix is
    [[1.01 1.95]
     [1.95 4.25]]



```python
batch_size=8
data_iter=DataLoader(data,batch_size=batch_size)
```

###  生成器Generator

我们的生成器网络将是最简单的网络-单层线性模型。这是因为我们将使用高斯数据生成器来驱动线性网络。因此，它只需要学习参数就可以完美地伪造生成数据。


```python
class net_G(nn.Module):
    def __init__(self):
        super(net_G,self).__init__()
        self.model=nn.Sequential(
            nn.Linear(2,2),
        )
        self._initialize_weights()
    def forward(self,x):
        x=self.model(x)
        return x
    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m,nn.Linear):
                m.weight.data.normal_(0,0.02)
                m.bias.data.zero_()
```

###  判别器Discriminator

For the discriminator we will be a bit more discriminating: we will use an MLP with 3 layers to make things a bit more interesting.


```python
class net_D(nn.Module):
    def __init__(self):
        super(net_D,self).__init__()
        self.model=nn.Sequential(
            nn.Linear(2,5),
            nn.Tanh(),
            nn.Linear(5,3),
            nn.Tanh(),
            nn.Linear(3,1),
            nn.Sigmoid()
        )
        self._initialize_weights()
    def forward(self,x):
        x=self.model(x)
        return x
    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m,nn.Linear):
                m.weight.data.normal_(0,0.02)
                m.bias.data.zero_()
```

###  训练

对于判别器，:我们将使用一个3层的MLP使事情变得更有趣。


```python
# Saved in the d2l package for later use
def update_D(X,Z,net_D,net_G,loss,trainer_D):
    batch_size=X.shape[0]
    Tensor=torch.FloatTensor
    ones=Variable(Tensor(np.ones(batch_size))).view(batch_size,1)
    zeros = Variable(Tensor(np.zeros(batch_size))).view(batch_size,1)
    real_Y=net_D(X.float())
    fake_X=net_G(Z)
    fake_Y=net_D(fake_X)
    loss_D=(loss(real_Y,ones)+loss(fake_Y,zeros))/2
    loss_D.backward()
    trainer_D.step()
    return float(loss_D.sum())
```

生成器的更新也类似. 这里我们仍使用交叉熵损失 cross-entropy loss，但是将 fake data 标签从 $0$ 变为 $1$.


```python
# Saved in the d2l package for later use
def update_G(Z,net_D,net_G,loss,trainer_G):
    batch_size=Z.shape[0]
    Tensor=torch.FloatTensor
    ones=Variable(Tensor(np.ones((batch_size,)))).view(batch_size,1)
    fake_X=net_G(Z)
    fake_Y=net_D(fake_X)
    loss_G=loss(fake_Y,ones)
    loss_G.backward()
    trainer_G.step()
    return float(loss_G.sum())
```

判别器和生成器都执行具有交叉熵损失的二元逻辑回归。我们用`Adam`来平滑训练过程。在每个迭代中，我们首先更新判别器，然后更新生成器。我们可视化损失和生成的例子。


```python
def train(net_D,net_G,data_iter,num_epochs,lr_D,lr_G,latent_dim,data):
    loss=nn.BCELoss()
    Tensor=torch.FloatTensor
    trainer_D=torch.optim.Adam(net_D.parameters(),lr=lr_D)
    trainer_G=torch.optim.Adam(net_G.parameters(),lr=lr_G)
    plt.figure(figsize=(7,4))
    d_loss_point=[]
    g_loss_point=[]
    d_loss=0
    g_loss=0
    for epoch in range(1,num_epochs+1):
        d_loss_sum=0
        g_loss_sum=0
        batch=0
        for X in data_iter:
            batch+=1
            X=Variable(X)
            batch_size=X.shape[0]
            Z=Variable(Tensor(np.random.normal(0,1,(batch_size,latent_dim))))
            trainer_D.zero_grad()
            d_loss = update_D(X, Z, net_D, net_G, loss, trainer_D)
            d_loss_sum+=d_loss
            trainer_G.zero_grad()
            g_loss = update_G(Z, net_D, net_G, loss, trainer_G)
            g_loss_sum+=g_loss
        d_loss_point.append(d_loss_sum/batch)
        g_loss_point.append(g_loss_sum/batch)
    plt.ylabel('Loss', fontdict={'size': 14})
    plt.xlabel('epoch', fontdict={'size': 14})
    plt.xticks(range(0,num_epochs+1,3))
    plt.plot(range(1,num_epochs+1),d_loss_point,color='orange',label='discriminator')
    plt.plot(range(1,num_epochs+1),g_loss_point,color='blue',label='generator')
    plt.legend()
    plt.show()
    print(d_loss,g_loss)
    
    Z =Variable(Tensor( np.random.normal(0, 1, size=(100, latent_dim))))
    fake_X=net_G(Z).detach().numpy()
    plt.figure(figsize=(3.5,2.5))
    plt.scatter(data[:,0],data[:,1],color='blue',label='real')
    plt.scatter(fake_X[:,0],fake_X[:,1],color='orange',label='generated')
    plt.legend()
    plt.show()
```

现在我们指定用来生成 Gaussian distribution的超参数 hyper-parameters 。


```python
if __name__ == '__main__':
    lr_D,lr_G,latent_dim,num_epochs=0.05,0.005,2,20
    generator=net_G()
    discriminator=net_D()
    train(discriminator,generator,data_iter,num_epochs,lr_D,lr_G,latent_dim,data)
```


<img src="/images/GAN_Basic/gan_3.png">


    0.6932446360588074 0.6927103996276855



<img src="/images/GAN_Basic/gan_4.png">


### 小结

* 对抗生成网络Generative adversarial networks (GANs)由两个网络组成： 生成网络the generator 和判别网络 the discriminator.
* 通过最大化交叉熵损失，使该生成器生成的图像尽可能地接近真实图像，以欺骗鉴别器， *i.e.*, $\max \log(D(\mathbf{x'}))$.
* 判别器试图通过最小化交叉熵损失来区分生成的图像和真实图像，, *i.e.*, $\min - y \log D(\mathbf{x}) - (1-y)\log(1-D(\mathbf{x}))$.


